# House-Price-Prediction
This is an Machine Learning model which predicts the house price. In this project I have used Boston Dataset to train and fit different regression model to predict the House Prices.<br>
# Data Information
Boston House Prices Dataset was collected in 1978 and has 506 entries with 14 attributes or features for homes from various suburbs in Boston.<br>
<br>
Boston Housing Dataset Attribute Information (in order):<br>
<table>
        <tr><td>- CRIM</td><td>per capita crime rate by town</td></tr>
        <tr><td>- ZN</td><td>proportion of residential land zoned for lots over 25,000 sq.ft.</td></tr>
        <tr><td>- INDUS</td><td>proportion of non-retail business acres per town</td></tr>
        <tr><td>- CHAS</td><td>Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)</td></tr>
        <tr><td>- NOX</td><td>nitric oxides concentration (parts per 10 million)</td></tr>
        <tr><td>- RM </td><td>average number of rooms per dwelling</td></tr>
        <tr><td>- AGE</td><td>proportion of owner-occupied units built prior to 1940</td></tr>
        <tr><td>- DIS</td><td>weighted distances to five Boston employment centres</td></tr>
        <tr><td>- RAD</td><td>index of accessibility to radial highways</td></tr>
        <tr><td>- TAX</td><td>full-value property-tax rate per $10,000</td></tr>
        <tr><td>- PTRATIO</td><td>pupil-teacher ratio by town</td></tr>
        <tr><td>- B </td><td>1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town</td></tr>
        <tr><td>- LSTAT </td><td>% lower status of the population</td></tr>
        <tr><td>- MEDV</td><td>Median value of owner-occupied homes in $1000's</td></tr>
</table>

<h3><strong>Download Link:</strong></h3>
<a href="https://www.kaggle.com/puxama/bostoncsv">https://www.kaggle.com/puxama/bostoncsv</a><br>
<br>

# Libraries
<ul>
<li>scikit-learn</li>
<li>Pandas</li>
<li>matplotlib</li>
<li>seaborn</li>
</ul><br>

# Algorithms
<ul>
<li>Linear Regression</li>
<li>Decision Tree Regression</li>
<li>Random Forest Regression</li>
<li>XGBoost Regression</li>
</ul><br>

# Result Summary:
<table>
<tr><th></th><th>Linear Regresson</th><th>Decision Tree</th><th>Random Forest</th><th>XGBoost</th></tr>
<tr><td>Mean Squared Error</td><td>23.87</td><td>11.39</td><td>10.06</td><td>10.03</td></tr>
<tr><td>Mean Absolute Error</td><td>3.18</td><td>2.57</td><td>2.13</td><td>2.11</td></tr>
